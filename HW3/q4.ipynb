{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4D0wd-mY3xc6"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Instruction: use these hyperparameters for both (b) and (d)\n",
    "eta = 0.5\n",
    "C = 5\n",
    "iterNums = [5, 50, 100, 1000, 5000, 6000]\n",
    "\n",
    "def svm_train_bgd(matrix, label, nIter):\n",
    "    # Implement your algorithm and return\n",
    "    state = {}\n",
    "    N, D = matrix.shape\n",
    "    \n",
    "    ############################\n",
    "    # Implement your code here #\n",
    "    w = np.zeros((D,1))\n",
    "    w = w.reshape(-1,1)\n",
    "    b = 0\n",
    "    #print(N, D)\n",
    "    for j in range(nIter):\n",
    "        sum_w = np.zeros((D,1))\n",
    "        sum_b = 0\n",
    "        for i in range(N):\n",
    "            #print((label[i]*(w.T.dot(matrix[i,:]) + b)).shape)\n",
    "            if (label[i]*(w.T.dot(matrix[i,:]) + b)) < 1:\n",
    "                sum_w += (label[i]*matrix[i, :]).reshape(-1,1)\n",
    "                sum_b += label[i]\n",
    "        grad_w = w - C*sum_w\n",
    "        grad_b = -C*sum_b\n",
    "        w = w - (eta/(1 + j*eta))*grad_w\n",
    "        b = b - 0.01*(eta/(1 + j*eta))*grad_b\n",
    "        #w = w_new\n",
    "        #b = b_new\n",
    "    print('\\nFor Iter %i' %nIter)\n",
    "    print('parameter w is:')\n",
    "    print(w)\n",
    "    print('parameter b is:')\n",
    "    print(b)\n",
    "    state['w'] = w\n",
    "    state['b'] = b\n",
    "    ############################\n",
    "    \n",
    "    return state\n",
    "\n",
    "def svm_train_sgd(matrix, label, nIter):\n",
    "    # Implement your algorithm and return\n",
    "    state = {}\n",
    "    N, D = matrix.shape\n",
    "    w = np.zeros((D,1))\n",
    "    w = w.reshape(-1,1)\n",
    "    b = 0\n",
    "    #print(N, D)\n",
    "    for j in range(nIter):\n",
    "        for i in range(N):\n",
    "            sum_w = np.zeros((D,1))\n",
    "            sum_b = 0\n",
    "            #print((label[i]*(w.T.dot(matrix[i,:]) + b)).shape)\n",
    "            if (label[i]*(w.T.dot(matrix[i,:]) + b)) < 1:\n",
    "                sum_w = (label[i]*matrix[i, :]).reshape(-1,1)\n",
    "                sum_b = label[i]\n",
    "            grad_w = w/N - C*sum_w\n",
    "            grad_b = -C*sum_b\n",
    "            w = w - (eta/(1 + j*eta))*grad_w\n",
    "            b = b - 0.01*(eta/(1 + j*eta))*grad_b\n",
    "        #w = w_new\n",
    "        #b = b_new\n",
    "    print('\\nFor Iter %i' %nIter)\n",
    "    print('parameter w is:')\n",
    "    print(w)\n",
    "    print('parameter b is:')\n",
    "    print(b)\n",
    "    state['w'] = w\n",
    "    state['b'] = b\n",
    "    \n",
    "    ############################\n",
    "    # Implement your code here #\n",
    "    ############################\n",
    "    \n",
    "    return state\n",
    "\n",
    "def svm_test(matrix, state):\n",
    "    # Classify each test data as +1 or -1\n",
    "    output = np.ones( (matrix.shape[0], 1) )\n",
    "    \n",
    "    ############################\n",
    "    # Implement your code here #\n",
    "    w = state['w']\n",
    "    b = state['b']\n",
    "    y = w.T.dot(matrix.T) + b\n",
    "    #print(matrix.shape)\n",
    "    #print(y.shape)\n",
    "    y[y>=1] = 1\n",
    "    y[y<=-1] = -1\n",
    "    #print(y)\n",
    "    output = y.reshape(-1,1)\n",
    "    #print(output)\n",
    "    \n",
    "    ############################\n",
    "    \n",
    "    return output\n",
    "\n",
    "def evaluate(output, label, nIter):\n",
    "    # Use the code below to obtain the accuracy of your algorithm\n",
    "    accuracy = (label * output > 0).sum() * 1. / len(output)\n",
    "    print('[Iter {:4d}: accuracy = {:2.4f}%'.format(nIter, 100*accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2ha6iX-Y3xdC"
   },
   "outputs": [],
   "source": [
    "# Note1: label is {-1, +1}\n",
    "# Note2: data matrix shape  = [Ndata, 4]\n",
    "# Note3: label matrix shape = [Ndata, 1]\n",
    "\n",
    "# Load data\n",
    "q4_data = np.load('q4_data/q4_data.npy', allow_pickle=True).item()\n",
    "\n",
    "train_x = q4_data['q4x_train']\n",
    "train_y = q4_data['q4y_train']\n",
    "\n",
    "\n",
    "test_x = q4_data['q4x_test']\n",
    "test_y = q4_data['q4y_test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hokrJWbI82rp"
   },
   "source": [
    "(b) Implement SVM using **batch gradient descent**. Print out the followings:\n",
    "\n",
    "*   Parameter w\n",
    "*   Parameter b\n",
    "*   Test accuracy (%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kQYkcSk98uUW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "For Iter 5\n",
      "parameter w is:\n",
      "[[112.  ]\n",
      " [-42.75]\n",
      " [272.5 ]\n",
      " [103.  ]]\n",
      "parameter b is:\n",
      "[-0.12416667]\n",
      "[Iter    5: accuracy = 54.1667%\n",
      "\n",
      "For Iter 50\n",
      "parameter w is:\n",
      "[[ -2.01960784]\n",
      " [-11.94117647]\n",
      " [ 25.85294118]\n",
      " [ 11.54901961]]\n",
      "parameter b is:\n",
      "[-0.37280358]\n",
      "[Iter   50: accuracy = 95.8333%\n",
      "\n",
      "For Iter 100\n",
      "parameter w is:\n",
      "[[-2.55940594]\n",
      " [-5.28217822]\n",
      " [11.37623762]\n",
      " [ 5.75742574]]\n",
      "parameter b is:\n",
      "[-0.38285]\n",
      "[Iter  100: accuracy = 95.8333%\n",
      "\n",
      "For Iter 1000\n",
      "parameter w is:\n",
      "[[-0.46353646]\n",
      " [-0.32617383]\n",
      " [ 1.05394605]\n",
      " [ 1.27872128]]\n",
      "parameter b is:\n",
      "[-0.40401205]\n",
      "[Iter 1000: accuracy = 95.8333%\n",
      "\n",
      "For Iter 5000\n",
      "parameter w is:\n",
      "[[-0.32083583]\n",
      " [-0.27904419]\n",
      " [ 0.89262148]\n",
      " [ 0.98660268]]\n",
      "parameter b is:\n",
      "[-0.4184513]\n",
      "[Iter 5000: accuracy = 95.8333%\n",
      "\n",
      "For Iter 6000\n",
      "parameter w is:\n",
      "[[-0.32919513]\n",
      " [-0.28186969]\n",
      " [ 0.886019  ]\n",
      " [ 0.97483753]]\n",
      "parameter b is:\n",
      "[-0.4199084]\n",
      "[Iter 6000: accuracy = 95.8333%\n"
     ]
    }
   ],
   "source": [
    "for nIter in iterNums:\n",
    "  # Train\n",
    "  state = svm_train_bgd(train_x, train_y, nIter)\n",
    "\n",
    "  # Test and evluate\n",
    "  prediction = svm_test(test_x, state)\n",
    "  evaluate(prediction, test_y, nIter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZcgdTdAp8_Zv"
   },
   "source": [
    "(d) Implement SVM using **stochastic gradient descent**. Print out the followings:\n",
    "\n",
    "*   Parameter w\n",
    "*   Parameter b\n",
    "*   Test accuracy (%)\n",
    "\n",
    "[Note: use the same hyperparameters as (b)]\n",
    "\n",
    "[Note: if you implement it correctly, the running time will be ~15 sec]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DkDUnCNz9G2S"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "For Iter 5\n",
      "parameter w is:\n",
      "[[-1.78136842]\n",
      " [-3.12818738]\n",
      " [ 8.55400016]\n",
      " [ 5.20287663]]\n",
      "parameter b is:\n",
      "[-0.05416667]\n",
      "[Iter    5: accuracy = 95.8333%\n",
      "\n",
      "For Iter 50\n",
      "parameter w is:\n",
      "[[-1.37946899e+00]\n",
      " [ 9.07974830e-04]\n",
      " [ 2.58689377e+00]\n",
      " [ 2.85570760e+00]]\n",
      "parameter b is:\n",
      "[-0.08671111]\n",
      "[Iter   50: accuracy = 95.8333%\n",
      "\n",
      "For Iter 100\n",
      "parameter w is:\n",
      "[[-1.25745166]\n",
      " [ 0.11439094]\n",
      " [ 1.70851556]\n",
      " [ 2.31719145]]\n",
      "parameter b is:\n",
      "[-0.09433571]\n",
      "[Iter  100: accuracy = 95.8333%\n",
      "\n",
      "For Iter 1000\n",
      "parameter w is:\n",
      "[[-0.48895966]\n",
      " [-0.18986655]\n",
      " [ 0.95735748]\n",
      " [ 1.14001054]]\n",
      "parameter b is:\n",
      "[-0.12014856]\n",
      "[Iter 1000: accuracy = 95.8333%\n",
      "\n",
      "For Iter 5000\n",
      "parameter w is:\n",
      "[[-0.42761221]\n",
      " [-0.23477963]\n",
      " [ 0.88908395]\n",
      " [ 1.06544336]]\n",
      "parameter b is:\n",
      "[-0.13850557]\n",
      "[Iter 5000: accuracy = 95.8333%\n",
      "\n",
      "For Iter 6000\n",
      "parameter w is:\n",
      "[[-0.44211714]\n",
      " [-0.21435765]\n",
      " [ 0.90972215]\n",
      " [ 1.06365376]]\n",
      "parameter b is:\n",
      "[-0.14003648]\n",
      "[Iter 6000: accuracy = 95.8333%\n"
     ]
    }
   ],
   "source": [
    "for nIter in iterNums:\n",
    "  # Train\n",
    "  state = svm_train_sgd(train_x, train_y, nIter)\n",
    "\n",
    "  # Test and evluate\n",
    "  prediction = svm_test(test_x, state)\n",
    "  evaluate(prediction, test_y, nIter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "q4.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
